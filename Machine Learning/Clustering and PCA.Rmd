---
title: "Machine Learning - Unsupervised Learning"
author: "Charles Leahan"
date: "11/18/2019"
output:
  pdf_document: default
  word_document: default
---

```{r, echo=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
```

# Clustering 

```{r}
head(faithful,10)
```

```{r}
#Visualization 

faithful %>% ggplot(aes(y =eruptions, x = waiting)) +
  geom_point()
```

```{r}
#Determin the optimal number of clusters 

library(factoextra)
fviz_nbclust(faithful, kmeans, method = "gap_stat")
```

```{r}
#Plot 

faithful.km <- faithful %>% mutate(cluster = factor(kmeans(x = . ,centers=2)$cluster))

faithful.km %>% ggplot(aes(waiting,eruptions,color = cluster)) + geom_point()
```

# Partitioning clustering vs K-Means

Want to cluster among states

```{r}
data("USArrests")
head(USArrests)
```

```{r}
#Scale the Data

USArrests1 <- USArrests %>% 
  na.omit() %>% 
  scale()

head(USArrests1)
```

```{r}
#Determine the optimal number of Clusters

library(factoextra)
fviz_nbclust(USArrests1, kmeans, method = "gap_stat")
```

```{r}
#K-Means
set.seed(100)
USArrests.km <- kmeans(USArrests1, 3)
fviz_cluster(USArrests.km,palette="jco",data=USArrests1)
```

```{r}
#PAM Method
set.seed(100)
USArrests.pam <- pam(USArrests1, 3)
fviz_cluster(USArrests.pam)
```

# Hierarchical Clustering

Not required to pre-specify the number of clusters to be generated

The Result of hierarichal clustering is a tree (sometimes called a dendrogram)

Two steps in constructing a tree 

1. Represent each case as point in a Cartesian space

2. Make branching decision based on how close together points or clouds of points are 


```{r}

fviz_dend(res.hc, k = 3, cex = 0.5, # label size
          k_colors = c("#2E9FDF", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE # Add rectangle around groups
          )
```


# Dimension Reduction 

## PCA

Create linear combinations of variables st the new varaibles are linearly uncorrelated 

PC1 has the largest possible varaince (accounts for as much of the variability in the data as possible), each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components 

The resulting vectors are an uncorrelated othogonal basis set

PCA is sensitive to the relative scaling of the original variables (variables with larger magnitude will have higher wieght in the PCA)


```{r}

#Need to remove response variable, then do principal component

pc.iris <- iris %>% select(-Species) %>% prcomp()

pc.iris$rotation[,1] #Coefficients for PC1
pc.iris$rotation[,2] #Coefficients for PC2
```

rotation contains all the betas for all the principal compponents 

Can  USE PC1 and PC2 to do clustering

```{r}
# Visualize PC1 and PC2

pc.iris$x %>% as_tibble() %>% bind_cols(iris) %>% 
  ggplot(aes(x = `PC1`, y = `PC2`, color = Species)) + geom_point()
```

# Example 2 

Want to group members

```{r, warning=False}

library(mdsr)
library(tidyverse)

wide.Votes <- Votes %>%
  spread(key = bill, value = vote) %>%
  as_tibble()

head(wide.Votes)
```

# Initial Approaches

```{r}
wide.Votes %>% select(c("S1M-240.2", "S1M-639.1")) %>% 
  table()
```

```{r}
wide.Votes %>% select(c("S1M-240.2", "S1M-639.1")) %>%
  ggplot(aes(y = `S1M-240.2`, x = `S1M-639.1`)) +
    geom_point(alpha = 0.7,
               position = position_jitter(width = 0.1, height = 0.1)) +
    geom_point(alpha = 0.01, size = 10, color = "red" ) 
```

```{r}
pc.Votes <- wide.Votes %>% select(-name) %>% prcomp()

pc.Votes$x %>% as_tibble() %>%   
  bind_cols(wide.Votes) %>% select(name,PC1,PC2) %>% head(10)
```




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 4, fig.height = 2)
```

