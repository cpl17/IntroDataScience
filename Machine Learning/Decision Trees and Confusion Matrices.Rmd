---
title: "ML Supervised: Decision Trees"
author: "Charles Leahan"
date: "11/6/2019"
output: word_document
---

```{r, echo = FALSE}
#Load Data 


library(tidyverse)

primary <- read.csv('C:/Users/CPL17/Python/data_sets/primaries.csv') 

#Clean and Attach Data 

primary <- primary %>% mutate(AFprop = black06 / pop06 * 100) %>% 
  filter(!is.na(AFprop)) %>%
  filter(!is.na(winner))

attach(primary)
```

# Decision Tree

Each branch separates the records in the data set into increasingly "pure" (homogeneous) subsets, in the sense that they are more likely to share the same class label 


```{r, echo = FALSE}

#load libaries used for decision trees 

library(rpart)
library(rpart.plot)

```


```{r}
#Fit the Model

mod_tree1 <- rpart(winner ~ AFprop + popUnder30_00, data = primary)

mod_tree1

#Plot the Tree

rpart.plot(mod_tree1)

```

_Model Interpretation_ 

Each line of the ouput represents a feature/node. 

Output is

1. split condition
2. number of observations
3. number of observations misclassified (loss) if using this model
3. value marked as 1/ y val 
4. (1-p,p) where p is probability of 1/yval winning at this node = # of obamas/ n

__Plot Interpretation__

Each node in the plot shows:

* The predicted class (Clinton or Obama)
* The predicted probability of Obama (Winner = 1)
* The percent of observations in the node 

This portion of the data comprises 19% of the data. 

A new observation with AFprop > 20, has a .84 predicted probability of voting for obama. 



```{r}
#Save the Formula

form_tree <- as.formula("winner ~ AFprop + region + pres04winner + pct_hs_grad")

#Fit and Plot the model, with a low 

mod_tree3 <- rpart (form_tree, data = primary, control = rpart.control(cp= 0.005))
rpart.plot(mod_tree3)
```

__Interpretation__

Since we lowered the control parameter(lowering the necessary reduction in class error rate) from the default .1 
to .005, less nodes are pruned, more nodes are in the model. 



# Confusion Matrix 



```{r}

#Building Confusing Matrix 

confusion_matrix3 <- primary %>% 
  mutate(pred3 = predict(mod_tree3, type = "class")) %>% 
  select(winner,pred3) %>% table()

#Calculating Accuracy

sum(diag(confusion_matrix3))/sum(confusion_matrix3)


```



# Training and Testing Data 

Never use the same data to fit and asses the model

In predictive analysis, data sets are often divided into two sets:

Training Set - The set of dta on which you build your model

Testing Set - Test the model by evaluation it against data it hasn't seen 

```{r}

library(dplyr)
set.seed(100)

#Divide the Data into train and testing data

train <- primary %>% sample_frac(size = .8)
test <- primary %>% setdiff(train)
```

```{r}
nrow(intersect(train,test))
```


# Model Evaluation 1 : Misclassification Rate

Goal: 

Compare the misclassification rate of two models with different control parameters. 

Steps: 

1. Fit the Models
2. Define a function that computes a confusion matrix 
3. Define a function that computes the misclassification rate
4. Compare the Test and Train Confusion Matrices for each model
5. Create matrix of misclassification rates 



```{r}
# Create fomula for predicting winner

form2 = as.formula("winner ~ POP05_SQMI + region + racetype + popUnder30_00+ AFprop + 
pop65up_00 + presVote04 + Bush04 + pop06 + white06 + pct_less_30k + pct_homeowner + unempChg")

# Create decision trees with different tuning parameters (cp)

mod_tree4 <- rpart(form2, data = train, 
              control = rpart.control(cp = 0.005))

mod_tree5 <- rpart(form2 , data = train, 
              control = rpart.control(cp = 0.0001)) #More complicated
```



```{r}

# Define function that computes confusion matrix 

confusion_matrix <- function(data,mod){
  confusion_matrix <- data %>% 
  mutate(pred = predict(mod, newdata = data, type = "class")) %>%
  select(winner,pred) %>% table()
}

# Define function that computes misclassification rate

misclass <- function(confusion){
  misclass <- 1- sum(diag(confusion))/sum(confusion)
}
```




```{r}
# Compute confusion matrix for test and train


  # Using tree4

confusion_tree4_test <- confusion_matrix(test,mod_tree4)
confusion_tree4_train <- confusion_matrix(train,mod_tree4)


  # Using tree5

confusion_tree5_test <- confusion_matrix(test,mod_tree5)
confusion_tree5_train <- confusion_matrix(train,mod_tree5)
```


```{r}

#Compute Train and Test Misclassification Error for each model 

tibble( 
  model = c("tree4","tree5"),
  trainerror = c(misclass(confusion_tree4_train),misclass(confusion_tree5_train)),
  testerror = c(misclass(confusion_tree4_test), misclass(confusion_tree5_test))
)
```



# Notes 


## Decision Tree

Assigns class lables to individual observations. Each branch separates the data set into increasingly more "pure" subsets - more likely to share the same class label.

__How to construct__

1. The number of possible decision trees grows exponentially wrt the nmber of explananotry variables

2. Computationally finding the optimal tree is impossible 

3. Thre are several competing heuristics for building decision trees that employ greed (locally optimal) strategies

4. Discussion restricted to recursive partitioning decision trees 

__Hunt's Algorithm__

The partitioning in a decision tree follows Huntâ€™s algorithm. Let Dt be the set of records that are associated with node t and y={y1,y2} be the class labels.

Step 1: If all the records in Dt belong to the same class yt, then t is a leaf node labeled as yt.

Step 2: If Dt contains records that belong to more than one class, partition the records into at least two child nodes, in such a way that the purity of the new set of nodes exceeds some threshold.

__Gini Coefficient__

Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according the the distribution of lables in the subset 

if p_i is the fraction of records that belong to class  i = 1:c, then 
Gini = 1 - sum(p_i^2)

Want to minimize Gini


__Information Gain__

Function that satisfies 

1. if P(Event) =1 then function gives 0 

2. if P(Event) = 0 then function gives high number  

4. if P(Event) = .5 then gives one bit of information 

A measure that satisfies these constraints is 

I(x) = -log2(p)


Examples:

1. Flipping Coin - 

-log2(.5) = 1 

2. Suppose meteor strikes the Earth with prob 2 ^(-22)

-log2(2^-22) = 22 

3. Prob sun will rise = 1 

-log(1) = 0 


__Entropy__

The expected value of the interesting-ness I(Y) of an event 

H(Y) = EV[I(Y)]



Example:

Suppose there are only two possible results for Y 

Y = 1 with prob p 

Then, 

H(Y) = - p*log_2(p) - (1-p)log_2(1-p) is the expected value of information

The information gain is the change in entropy. 

General formula: For c classes

H(y) = - sum(p_i*log_2(p_i))


Higher entropy implies low purity 


__Tuning Parameters__

The complexity paramet, controls wheter to keep or prune possible splits 

The algorithm prunes the possible splits if they do not suffiecient add to predictivity 

Higher the cp, less complicated model (bound on complexity )

__Measuring Classification Performance__ 

A confusion matrix is a two way table that counts how of ten our model made the correct prediction 

Two types of errors 

* Predicting winner is clinton when obama 
* predictin winner is obama when clinton













```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


   